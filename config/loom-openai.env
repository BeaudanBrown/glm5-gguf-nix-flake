# Loom Server â€” OpenAI-compatible backend configuration.
#
# Add these to your loom-server's environment on your NixOS box to point
# it at the HPC's llama-server endpoint via Tailscale.
#
# The exact env var names depend on how loom-server is configured in your
# NixOS module. These are the standard OpenAI-compatible variables.

# Point at the HPC node running llama-server via Tailscale.
# Use MagicDNS name (hpc-glm5) or the Tailscale IP.
OPENAI_API_BASE=http://hpc-glm5:8080/v1

# llama-server doesn't require an API key by default, but the client
# libraries usually require *something* to be set.
OPENAI_API_KEY=not-needed

# Model ID as reported by llama-server's /v1/models endpoint.
# This is typically the filename of the GGUF.
# OPENAI_MODEL=glm5
