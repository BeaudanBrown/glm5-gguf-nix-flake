#!/usr/bin/env bash
#SBATCH --job-name=glm5-server
#SBATCH --time=24:00:00
#SBATCH --gres=gpu:l40s:4
#SBATCH --cpus-per-task=96
#SBATCH --mem=512G
#SBATCH --output=logs/glm5-%j.out
#SBATCH --error=logs/glm5-%j.err
#
# slurm-batch.sh — Submit a batch job that runs the full inference stack.
#
# Usage (from the project root on the HPC login node):
#   mkdir -p logs          # must exist before sbatch reads the #SBATCH paths
#   sbatch scripts/slurm-batch.sh
#
# The job will:
#   1. Source nixsa-gpu-setup.sh to expose NVIDIA devices to the Nix sandbox
#   2. Enter the Nix devShell
#   3. Start Tailscale (userspace-networking — no root, no /dev/net/tun required)
#   4. Start llama-server bound to the Tailscale IP
#   5. Write .connection-info so you can find the API URL
#   6. Run until the job time limit expires or you scancel it
#
# Monitor with:
#   tail -f logs/glm5-<jobid>.out
#   squeue -u $USER
#
# Connect from your local machine once the server is up:
#   bash scripts/connect.sh
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(cd "$SCRIPT_DIR/.." && pwd)"

cd "$PROJECT_DIR"

echo "=== GLM-5 Batch Job ==="
echo "Job ID:   ${SLURM_JOB_ID:-unknown}"
echo "Node:     $(hostname)"
echo "Started:  $(date -Iseconds)"
echo ""

# Ensure the log directory exists (sbatch requires it before the job starts,
# so callers must run `mkdir -p logs` first — this is a belt-and-suspenders check).
mkdir -p "$PROJECT_DIR/logs"

# Source GPU passthrough for nixsa (exposes NVIDIA devices + /dev/net/tun to bwrap)
if [ -f "$PROJECT_DIR/nixsa-gpu-setup.sh" ]; then
  source "$PROJECT_DIR/nixsa-gpu-setup.sh"
fi

# Load secrets / overrides from .env
if [ -f "$PROJECT_DIR/.env" ]; then
  set -a; source "$PROJECT_DIR/.env"; set +a
fi

# Set env vars that the inner nix develop shell needs; export them so they
# survive the nix develop boundary.
export PROJECT_DIR
export TS_STATE_DIR="${TS_STATE_DIR:-$PROJECT_DIR/.tailscale-state}"
export TS_SOCKET="${TS_SOCKET:-$TS_STATE_DIR/tailscaled.sock}"
export TS_HOSTNAME="${TS_HOSTNAME:-hpc-glm5}"
export PORT="${PORT:-8080}"

echo "GPU inventory:"
nvidia-smi --query-gpu=index,name,memory.total --format=csv,noheader 2>/dev/null \
  || echo "  (nvidia-smi unavailable outside nixsa sandbox — normal at this stage)"
echo ""

# ── Cleanup: stop Tailscale when the job exits for any reason ──────────
cleanup() {
  echo ""
  echo "=== Shutting down ($(date -Iseconds)) ==="
  # tailscale is on PATH inside nix develop but not necessarily here;
  # use the state dir directly to kill the daemon we started.
  if [ -f "$TS_STATE_DIR/tailscaled.pid" ]; then
    PID=$(cat "$TS_STATE_DIR/tailscaled.pid")
    if kill -0 "$PID" 2>/dev/null; then
      echo "Stopping tailscaled (PID $PID)..."
      kill "$PID" 2>/dev/null || true
      sleep 2
      kill -0 "$PID" 2>/dev/null && kill -9 "$PID" 2>/dev/null || true
    fi
    rm -f "$TS_STATE_DIR/tailscaled.pid"
  fi
  rm -f "$TS_SOCKET"
  rm -f "$PROJECT_DIR/.connection-info"
  echo "Done."
}
trap cleanup EXIT INT TERM

# ── Main: run everything inside the nix devShell ───────────────────────
# Variables are exported above so they are visible inside the subprocess.
# Double-quote the heredoc delimiter (<<"EOF") so the outer shell expands
# variables before passing the script to `nix develop --command bash`.
nix develop --command bash -c "
  set -euo pipefail

  echo 'Nix devShell active.'
  echo ''

  # Start Tailscale (blocks until connected or exits non-zero)
  glm5-tailscale-up

  # Fetch the Tailscale IP for the connection-info file
  TS_IP=\$(tailscale --socket=\"\$TS_SOCKET\" ip -4 2>/dev/null || echo 'unknown')

  # Write a machine-readable connection summary to the project dir so that
  # scripts/connect.sh and the user can find the endpoint quickly.
  cat > \"$PROJECT_DIR/.connection-info\" <<INFO
# Generated by SLURM job ${SLURM_JOB_ID:-unknown} on \$(date -Iseconds)
# This file is auto-generated — do not edit.
SLURM_JOB_ID=${SLURM_JOB_ID:-unknown}
NODE=\$(hostname)
TS_IP=\$TS_IP
TS_HOSTNAME=$TS_HOSTNAME
PORT=$PORT
API_URL=http://\$TS_IP:$PORT/v1
HEALTH_URL=http://\$TS_IP:$PORT/health
INFO

  echo ''
  echo '=== Connection info ==='
  cat \"$PROJECT_DIR/.connection-info\"
  echo ''
  echo 'From your local machine run:  bash scripts/connect.sh'
  echo ''

  # Start the inference server — this blocks until the job is cancelled.
  # HOST is resolved inside glm5-serve to the Tailscale IP automatically.
  glm5-serve
"
