# GLM-5 Inference Server — Environment Configuration
# Copy to .env and fill in your values:
#   cp .env.example .env

# ── Tailscale ───────────────────────────────────────────────────────────
# Auth key from https://login.tailscale.com/admin/settings/keys
# Use a reusable, ephemeral key for HPC jobs.
TS_AUTHKEY=tskey-auth-xxxxx

# Hostname on your tailnet (default: hpc-glm5)
# TS_HOSTNAME=hpc-glm5

# ── Model ───────────────────────────────────────────────────────────────
# Path to GGUF model file. Auto-detected from .models/ if unset.
# MODEL_PATH=./.models/gguf/GLM-5-UD-Q4_K_XL-00001-of-00010.gguf

# ── Server ──────────────────────────────────────────────────────────────
# PORT=8080
# CTX_SIZE=16384
# PARALLEL=2
# BATCH_SIZE=2048
# UBATCH_SIZE=512

# ── Optional overrides ──────────────────────────────────────────────────
# THREADS=48
# THREADS_BATCH=96
# EXTRA_ARGS=--verbose
