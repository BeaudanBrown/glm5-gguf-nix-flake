# GLM-5 Inference Server — Environment Configuration
# Copy to .env and fill in your values:
#   cp .env.example .env

# ── Tailscale / Headscale ───────────────────────────────────────────────
# Coordination server. Defaults to https://hs.bepis.lol (headscale).
# TS_LOGIN_SERVER=https://hs.bepis.lol

# Preauth key generated from your headscale instance.
# headscale preauthkeys create --reusable --ephemeral
TS_AUTHKEY=tskey-auth-xxxxx

# Hostname on your tailnet (default: hpc-glm5)
# TS_HOSTNAME=hpc-glm5

# ── Model ───────────────────────────────────────────────────────────────
# Path to GGUF model file. Auto-detected from .models/ if unset.
# MODEL_PATH=./.models/gguf/GLM-5-UD-Q4_K_XL-00001-of-00010.gguf

# ── Server ──────────────────────────────────────────────────────────────
# PORT=8080
# CTX_SIZE=16384
# PARALLEL=2
# BATCH_SIZE=2048
# UBATCH_SIZE=512

# ── Optional overrides ──────────────────────────────────────────────────
# THREADS=48
# THREADS_BATCH=96
# EXTRA_ARGS=--verbose
