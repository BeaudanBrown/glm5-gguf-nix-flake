# GLM-5 Inference Server — Environment Configuration
# Copy to .env and fill in your values:
#   cp .env.example .env

# ── Tailscale / Headscale ───────────────────────────────────────────────
# Coordination server. Defaults to https://hs.bepis.lol (headscale).
# TS_LOGIN_SERVER=https://hs.bepis.lol

# Preauth key generated from your headscale instance.
# headscale preauthkeys create --reusable --ephemeral
TS_AUTHKEY=tskey-auth-xxxxx

# Hostname on your tailnet (default: hpc-glm5)
# TS_HOSTNAME=hpc-glm5

# ── Model ───────────────────────────────────────────────────────────────
# Path to GGUF model file. Auto-detected from .models/ and cache/ if unset.
# Q4_K_XL is the recommended quant for 4x L40S (431 GB model, ~42% on GPU).
# Q8_K_XL (869 GB) gives negligible quality gain on a 744B MoE but halves
# inference speed since only ~21% of layers fit on GPU.
# MODEL_PATH=./.models/gguf/GLM-5-UD-Q4_K_XL-00001-of-00010.gguf

# ── Server ──────────────────────────────────────────────────────────────
# PORT=8080
# CTX_SIZE=65536
# PARALLEL=2
# BATCH_SIZE=2048
# UBATCH_SIZE=512

# ── KV cache quantisation ──────────────────────────────────────────────
# GLM-5 uses MLA (kv_lora_rank=512) so the KV cache is tiny (~78 KB/token).
# q8_0 halves it further with no measurable quality loss.  At 65K context
# with 2 parallel slots the KV cache is only ~5 GB per slot (~10 GB total).
# CACHE_TYPE_K=q8_0
# CACHE_TYPE_V=q8_0

# ── Optional overrides ──────────────────────────────────────────────────
# THREADS=48
# THREADS_BATCH=96
# EXTRA_ARGS=--verbose
